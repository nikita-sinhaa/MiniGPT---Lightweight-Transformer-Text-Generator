
# ğŸ§  MiniGPT - Lightweight Transformer Text Generator

## ğŸ¯ Project Overview
This project demonstrates how to build and train a minimal Transformer-based language model from scratch using PyTorch.

## ğŸ›  Features
- Tokenization and embedding
- Transformer Encoder (multi-head self-attention)
- Text generation
- Notebook and CLI-based exploration

## ğŸ§ª Dataset
- Tiny Shakespeare sample (~500 characters)

## ğŸ“¦ Structure
- `model/`: Transformer definition
- `training/`: Training loop and tokenizer
- `data/`: Input text
- `output/`: Generated text output
- `notebooks/`: Jupyter analysis

## ğŸ§‘â€ğŸ’» Author
Nikita Sinha â€“ Firmware, Embedded Systems & Deep Learning
