
# 🧠 MiniGPT - Lightweight Transformer Text Generator

## 🎯 Project Overview
This project demonstrates how to build and train a minimal Transformer-based language model from scratch using PyTorch.

## 🛠 Features
- Tokenization and embedding
- Transformer Encoder (multi-head self-attention)
- Text generation
- Notebook and CLI-based exploration

## 🧪 Dataset
- Tiny Shakespeare sample (~500 characters)

## 📦 Structure
- `model/`: Transformer definition
- `training/`: Training loop and tokenizer
- `data/`: Input text
- `output/`: Generated text output
- `notebooks/`: Jupyter analysis

## 🧑‍💻 Author
Nikita Sinha – Firmware, Embedded Systems & Deep Learning
